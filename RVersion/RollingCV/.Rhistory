source('~/Desktop/ToCompare/model_selection.r')
stopCluster(cl)
rm(list = ls())
suppressMessages(library(Matrix))
suppressMessages(library(quantreg))
suppressMessages(library(parallel))
suppressMessages(library(compiler))
suppressMessages(library(lars))
suppressMessages(library(elasticnet))
suppressMessages(library(caret))
options(warn=-1)
#####################################################################################
source('Auxiliar.r')
source('elastic_net_fit.r')
source('LOOCV.r')
source('KernelFunctions.r')
source('OutOfSample.r')
source('Interactions.R')
source('TrainingError.r')
################# Regularized S-map for inference and forecasting ###################
##### Few notes:
##### 1) Here we use elastic net regularization and three different kernel functions
##### 2) This is the most complete version of the code as it cross-validate
#####    every possible variable:
#####     a) kernel function
#####     b) ratio of L1 to L2 norm
#####     c) kernel bandwidth
#####     d) lambda
##### For the analysis of empirical data with multivariate time series one should also
##### cross validate features and time lags
##### Generally, the optimum kernel function and ratio of L1 to L2 norm (as well as features
##### and time lags in empirical data) are consistent across the time series.
##### So, if one were to repeat the inference and forecasting across multiple
##### chunks of the time series. then a) and b) can be selected on a small training
##### set and then kept fixed for the following analysis
##### On the other hand it is important to always cross validate c) and d)
##### -----------------------
##### Generally we suggest to run 'model_selection.r' (<--- very lengthy and slow)
##### Then get the optimum a) and b) and use them to run 'main.r' which is much faster
#####################################################################################
ShowPlot = F
Kernel.Options = c('Exponential.Kernel', 'Epanechnikov.Kernel', 'TriCubic.Kernel')
choice = 2
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
####################################
d.training = as.matrix(read.table('DataForComparison/TrainingData/TrainingData_Example_1.txt'))
Embedding = LETTERS[1:ncol(d.training)]
TargetList = Embedding
######################
dfdx = expand.grid(TargetList, TargetList)
####
RegressionType = 'ELNET_fit'
ratio = c(0.85, 0.9, 0.95, 1.)
choice = c(1,2,3)
other.parameters = expand.grid(ratio,choice)
val_err = c()
View(d.training)
#### Now run the fit in parallel
Lavoratori = detectCores() - 1
cl <- makeCluster(Lavoratori, type = "FORK")
for(k in 1:nrow(other.parameters)){
Regression.Kernel = Kernel.Options[other.parameters[k,2]]
lambda = logspace(-3,0,15)
if(Regression.Kernel == 'Exponential.Kernel'){
tht = logspace(-2,1.2,15)
}else{
tht = logspace(-2,1.2,15)
}
parameters_on_grid = expand.grid(tht, lambda)
BestModelElnet = LOO.CV(cl, d.training, TargetList, Embedding, parameters_on_grid,
RegressionType,other.parameters[k,1],Regression.Kernel)
val_err = c(val_err, BestModelElnet$validation.error)
}
idx = which(val_err == min(val_err))
Regression.Kernel = Kernel.Options[other.parameters[idx,1]]
lambda = logspace(-3,0,15)
if(Regression.Kernel == 'Exponential.Kernel'){
tht = logspace(-2,1.2,15)
}else{
tht = logspace(-2,1.2,15)
}
rm(list = ls())
suppressMessages(library(Matrix))
suppressMessages(library(quantreg))
suppressMessages(library(parallel))
suppressMessages(library(compiler))
suppressMessages(library(lars))
suppressMessages(library(elasticnet))
suppressMessages(library(caret))
options(warn=-1)
#####################################################################################
source('Auxiliar.r')
source('elastic_net_fit.r')
source('LOOCV.r')
source('KernelFunctions.r')
source('OutOfSample.r')
source('Interactions.R')
source('TrainingError.r')
################# Regularized S-map for inference and forecasting ###################
##### Few notes:
##### 1) Here we use elastic net regularization and three different kernel functions
##### 2) This is the most complete version of the code as it cross-validate
#####    every possible variable:
#####     a) kernel function
#####     b) ratio of L1 to L2 norm
#####     c) kernel bandwidth
#####     d) lambda
##### For the analysis of empirical data with multivariate time series one should also
##### cross validate features and time lags
##### Generally, the optimum kernel function and ratio of L1 to L2 norm (as well as features
##### and time lags in empirical data) are consistent across the time series.
##### So, if one were to repeat the inference and forecasting across multiple
##### chunks of the time series. then a) and b) can be selected on a small training
##### set and then kept fixed for the following analysis
##### On the other hand it is important to always cross validate c) and d)
##### -----------------------
##### Generally we suggest to run 'model_selection.r' (<--- very lengthy and slow)
##### Then get the optimum a) and b) and use them to run 'main.r' which is much faster
#####################################################################################
ShowPlot = F
Kernel.Options = c('Exponential.Kernel', 'Epanechnikov.Kernel', 'TriCubic.Kernel')
choice = 2
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
####################################
d.training = as.matrix(read.table('DataForComparison/TrainingData/TrainingData_Example_1.txt'))
Embedding = LETTERS[1:ncol(d.training)]
TargetList = Embedding
######################
dfdx = expand.grid(TargetList, TargetList)
####
RegressionType = 'ELNET_fit'
ratio = c(0.85, 0.9, 0.95, 1.)
choice = c(1,2,3)
other.parameters = expand.grid(ratio,choice)
val_err = c()
k = 1
#### Now run the fit in parallel
Lavoratori = detectCores() - 1
cl <- makeCluster(Lavoratori, type = "FORK")
Regression.Kernel = Kernel.Options[other.parameters[k,2]]
lambda = logspace(-3,0,15)
tht = logspace(-2,1.2,15)
parameters_on_grid = expand.grid(tht, lambda)
BestModelElnet = LOO.CV(cl, d.training, TargetList, Embedding, parameters_on_grid,
RegressionType,other.parameters[k,1],Regression.Kernel)
dfdx
stopCluster(cl)
d.training
typeof(d.training)
stopCluster(cl)
View(d.training)
source('~/Desktop/ToCompare/model_selection.r')
stopCluster(cl)
other.parameters[idx,1]
Regression.Kernel
Regression.Kernel = Kernel.Options[other.parameters[idx,1]]
View(other.parameters)
Regression.Kernel = Kernel.Options[other.parameters[idx,2]]
BestModelElnet = LOO.CV(cl, d.training, TargetList, Embedding, parameters_on_grid,
RegressionType,other.parameters[idx,1],Regression.Kernel)
#### Now run the fit in parallel
Lavoratori = detectCores() - 1
cl <- makeCluster(Lavoratori, type = "FORK")
BestModelElnet = LOO.CV(cl, d.training, TargetList, Embedding, parameters_on_grid,
RegressionType,other.parameters[idx,1],Regression.Kernel)
stopCluster(cl)
#### Coefficient of the Jacobian and intercept
BestCoefficients.elnet = BestModelElnet$BestCoefficients
#### Bandwith of the kernel and regularization parameter
BestParameters.elnet = BestModelElnet$BestParameters
### Out-of-sample forecast just to be 100% sure do not load the test data yet
length.testing = nrow(as.matrix(read.table(name.for.testing)))
out.of.samp = forecast.function(BestCoefficients.elnet, BestParameters.elnet$BestTH,
BestParameters.elnet$BestLM, other.parameters[idx,1],
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
###############################
###### Now check the in-sample error
TrainErr.elnet = ComputeTrainingError(d.training, BestCoefficients.elnet)
reconstruction = ReconstructionOfTrainingSet(d.training, BestCoefficients.elnet)
###### Now check the quality of the inference
rho.elnet = jacobiano_analitico(JacobianName,
BestCoefficients.elnet$J, ncol(d), t.min, t.min+length.training-1)
elnet.matrix.inference = rho.elnet$correlation.matrix
##### Only here we can take the test set out so that model selection did not use the test data
training.error.rho = TrainErr.elnet$rho
training.error.rmse = compute.rmse.train(d.training,reconstruction)
d.testing = as.matrix(read.table(name.for.testing))
colnames(d.testing) = Embedding
test.error.rho = MeanCorrelation(d.testing, prd)$correlation
test.error.rmse = MeanCorrelation(d.testing, prd)$rmse
##### Make a couple of plots to visualize the results
if(ShowPlot == TRUE){
source('PlotFunctions.r')
par(mfrow = c(ncol(d.training), 1))
for(i in 1:ncol(d.training)){
Put_train_test_together(d.training[2:nrow(d.training),],d.testing,
reconstruction, prd, i)
}
par(mfrow = c(1,1))
}
cat('Out of sample correlation coefficient:', test.error.rho, '\n')
cat('Out of sample root mean squared error:', test.error.rmse, '\n')
source('PlotFunctions.r')
par(mfrow = c(ncol(d.training), 1))
for(i in 1:ncol(d.training)){
Put_train_test_together(d.training[2:nrow(d.training),],d.testing,
reconstruction, prd, i)
}
par(mfrow = c(1,1))
